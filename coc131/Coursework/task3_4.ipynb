{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d0c07d",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "# Local imports\n",
    "from ipynb.fs.defs.task3_1 import DatasetManager\n",
    "from ipynb.fs.defs.task3_2 import ModelManager, plot_bar_data, plot_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de81a5b0",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e181339",
   "metadata": {},
   "source": [
    "## ModelManager Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelManager class is modified to accomodate new clustering models\n",
    "class ModelManager3(ModelManager):\n",
    "    def __init__(self, feature_set, targets):\n",
    "        super().__init__(feature_set, targets)\n",
    "    \n",
    "    def train_model_reg(self):\n",
    "        \"\"\"\n",
    "        Trains a KMeans clustering model for\n",
    "        regression tasks.\n",
    "        \"\"\"\n",
    "        # Getting training and test data\n",
    "        X_train = self._train_and_test_sets.get(\"X_train\")\n",
    "        y_train = self._train_and_test_sets.get(\"y_train\")\n",
    "        \n",
    "        X_test = self._train_and_test_sets.get(\"X_test\")\n",
    "        y_test = self._train_and_test_sets.get(\"y_test\")\n",
    "        \n",
    "        # Fit the KMeans model on the training data\n",
    "        print(\"Fitting model...\")\n",
    "        km = MiniBatchKMeans(n_clusters=50, batch_size=50)\n",
    "        km.fit(X_train)\n",
    "        print(\"Model fitting complete...\")\n",
    "\n",
    "        # Assign instances in training set to closest cluster and get mean target value of each cluster\n",
    "        print(\"Making predictions...\")\n",
    "        train_labels = km.predict(X_train)\n",
    "        cluster_means = [y_train[train_labels == i].mean() for i in range(km.n_clusters)]\n",
    "        train_preds = np.array([cluster_means[i] for i in train_labels])\n",
    "\n",
    "        # Predict clusters of the test set and assign mean target value of corresponding cluster to each instance\n",
    "        test_labels = km.predict(X_test)\n",
    "        test_preds = np.array([cluster_means[i] for i in test_labels])\n",
    "        \n",
    "        self._train_preds = train_preds\n",
    "        self._test_preds = test_preds\n",
    "        self._trained_model = km\n",
    "    \n",
    "    def train_model_clf(self, n_classes):\n",
    "        \"\"\"\n",
    "        Trains a KMeans clustering model for \n",
    "        classification tasks.\n",
    "        \"\"\"\n",
    "        assert self._train_and_test_sets != None, \"You don't have your training and test sets.\"\n",
    "        # Getting training and test data\n",
    "        X_train = self._train_and_test_sets.get(\"X_train\")\n",
    "        y_train = self._train_and_test_sets.get(\"y_train\")\n",
    "        \n",
    "        X_test = self._train_and_test_sets.get(\"X_test\")\n",
    "        y_test = self._train_and_test_sets.get(\"y_test\")\n",
    "        \n",
    "        # Fit the KMeans model on the training data\n",
    "        print(\"Fitting model...\")\n",
    "        km = MiniBatchKMeans(n_clusters=n_classes, batch_size=n_classes)\n",
    "        km.fit(X_train)\n",
    "        print(\"Model fitting complete...\")\n",
    "\n",
    "        # Predict clusters on the training set\n",
    "        print(\"Making predictions...\")\n",
    "        train_preds = km.predict(X_train)\n",
    "\n",
    "        # Predict clusters on the test set\n",
    "        test_preds = km.predict(X_test)\n",
    "\n",
    "        # Assign class variables\n",
    "        self._trained_model = km\n",
    "        self._train_preds = train_preds\n",
    "        self._test_preds = test_preds\n",
    "        \n",
    "    def visualise_results_clf(self):\n",
    "        \"\"\"\n",
    "        Creates a series of plots to visualise performance\n",
    "        results for a classification model.\n",
    "        \"\"\"\n",
    "        assert self._trained_model != None, \"You haven't trained a model yet.\"\n",
    "        # Getting training, test and predictions data\n",
    "        y_train = self._train_and_test_sets.get(\"y_train\")\n",
    "        y_test = self._train_and_test_sets.get(\"y_test\")\n",
    "        train_preds = self._train_preds\n",
    "        test_preds = self._test_preds\n",
    "\n",
    "        # Get key metric plot\n",
    "        key_metric_plot = self._get_key_metric_plot_clf(y_train, y_test, train_preds, test_preds)\n",
    "\n",
    "        # True values vs predictions\n",
    "        true_pred_plot = self._get_true_pred_plot(y_test, test_preds)\n",
    "\n",
    "        # Combining plots\n",
    "        plots = {\n",
    "            (1,1,\"Dataset\",\"\"): key_metric_plot,\n",
    "            (2,1,\"\",\"Class\"): true_pred_plot,\n",
    "        }\n",
    "\n",
    "        subplot_titles = [\n",
    "            \"Key Metrics\", \n",
    "            \"True vs Predicted Values\",\n",
    "        ]\n",
    "\n",
    "        specs = [\n",
    "            [{\"type\": \"bar\", \"colspan\": 2}, None],\n",
    "            [{\"type\": \"xy\", \"colspan\": 2}, None],\n",
    "        ]\n",
    "\n",
    "        combined_plot = plot_collection(\n",
    "            plots, \n",
    "            rows=2, \n",
    "            cols=2, \n",
    "            subplot_titles=subplot_titles, \n",
    "            specs=specs, \n",
    "            title=\"Model Performance Results\", \n",
    "        )\n",
    "\n",
    "        return combined_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a525d",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e735ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset; using optimal configuration as determined in Task3-1\n",
    "gwp_dsm = DatasetManager(\"gwp_assessment\")\n",
    "gwp_dsm.load_and_preprocess([0,1,2,3], \"iterative\")\n",
    "gwp_dsm.create_feature_set(7)\n",
    "gwp_dsm.scale_feature_set()\n",
    "\n",
    "# Star dataset; using optimal configuration as determined in Task3-1\n",
    "star_dsm = DatasetManager(\"star_assessment\")\n",
    "star_dsm.load_and_preprocess([0,1,8,9,12,16,17], \"knn\")\n",
    "star_dsm.create_feature_set(8)\n",
    "star_dsm.scale_feature_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a4dc5",
   "metadata": {},
   "source": [
    "## Getting Targets and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2316c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset\n",
    "gwp_features = gwp_dsm.get_scaled_feat_ds()\n",
    "gwp_targets = gwp_dsm.get_complete_ds()[:, -1]\n",
    "\n",
    "# Star dataset\n",
    "star_features = star_dsm.get_scaled_feat_ds()\n",
    "star_targets = star_dsm.get_complete_ds()[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35479835",
   "metadata": {},
   "source": [
    "## Initialising Model Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be691825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWP dataset\n",
    "gwp_mm = ModelManager3(gwp_features, gwp_targets)\n",
    "\n",
    "# Star dataset\n",
    "star_mm = ModelManager3(star_features, star_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74678831",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b629cf",
   "metadata": {},
   "source": [
    "**Methodology**\n",
    "1. Datasets will be split into training and test sets.\n",
    "2. Models will be trained on training sets; cross validation will be used to optimise hyperparameters.\n",
    "3. Model performance will be evaluated using selected evaluation metrics; the results will then be visualised to paint full picture of a model's performance.\n",
    "4. Steps 1-3 will be repeated for several training-test splits (80-20, 75-25, 70-30, 60-40, 50-50) to assess the effect of split ratio on model performance.\n",
    "\n",
    "**Evaluation metrics**\n",
    "- Productivity dataset: accuracy, precision, recall, F1 score. These metrics are ideal metrics for evaluating classification models as they provide comprehensive insight into a model's performance. Accuracy helps understand the overall effectiveness of the model. However, it can be misleading in imbalanced datasets, which is where precision and recall come in. They provide a more nuanced view of the model's ability to correctly identify positive instances and avoid false positives. The F1 score harmonises precision and recall, offering a single metric that seeks a balance between these two characteristics, making it especially useful when the costs of false positives and false negatives are significantly different.\n",
    "\n",
    "- Star dataset: mean squared error (MSE), mean abolute error (MAE), R2 score. These are robust metrics for evaluating regression models, with each illuminating different aspects of model performance. MSE emphasizes larger errors by squaring residuals, making it useful when larger errors are undesirable. MAE provides a more straightforward measure of average error magnitude, regardless of direction. The R2 score complements these by providing a relative measure of how much variance the model can explain, giving a broader picture of model performance beyond just raw error. These combined provide a comprehensive assessment of the model's effectiveness.\n",
    "\n",
    "**Notes**\n",
    "- Due to the size of the star dataset (as well as the limitations of the machine on which this program was developed) only small subset of the dataset (approximately 2%) will be used to train models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc1fc95",
   "metadata": {},
   "source": [
    "### 80-20 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2302be",
   "metadata": {},
   "source": [
    "#### Splitting Datasets into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting productivity dataset\n",
    "gwp_mm.split_dataset(train_size=0.8, test_size=0.2)\n",
    "\n",
    "# Splitting star dataset\n",
    "star_mm.split_dataset(train_size=0.016, test_size=0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf10a60",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset\n",
    "gwp_mm.train_model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.train_model_clf(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868189f7",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760fa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "gwp_mm.visualise_results_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84470c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.visualise_results_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5fe9f",
   "metadata": {},
   "source": [
    "#### Analysis (INCOMPLETE)\n",
    "**Productivity dataset**\n",
    "- The model acheives a satisfactorally low score across all error metrics (MSE and MAE) in both the training and test sets.\n",
    "- However, the R2 score suggests that the model is unlikely to generalise well to novel data and may therefore require more fine-tuning. \n",
    "- The R2 score is particularly low on the test set, which could be an indication of underfitting.\n",
    "\n",
    "**Star dataset**\n",
    "- The model's accuracy, precision and recall scores are consistent across both the training and test sets, with the model scoring only slighter better on the training set across all metrics.\n",
    "- The scores indicate that the model has neither over- or under- fitted on the training data and can generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc7817",
   "metadata": {},
   "source": [
    "### 75-25 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f2bdc",
   "metadata": {},
   "source": [
    "#### Splitting Datasets into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b51c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting productivity dataset\n",
    "gwp_mm.split_dataset(train_size=0.75, test_size=0.25)\n",
    "\n",
    "# Splitting star dataset\n",
    "star_mm.split_dataset(train_size=0.015, test_size=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b029703",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be491ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset\n",
    "gwp_mm.train_model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa17722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.train_model_clf(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fd5d6",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "gwp_mm.visualise_results_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.visualise_results_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251dbd8",
   "metadata": {},
   "source": [
    "#### Analysis (INCOMPLETE)\n",
    "**Productivity dataset**\n",
    "- The model acheives a satisfactorally low score across all error metrics (MSE and MAE) in both the training and test sets.\n",
    "- However, the R2 score suggests that the model is unlikely to generalise well to novel data and may therefore require more fine-tuning. \n",
    "- The R2 score is particularly low on the test set, which could be an indication of underfitting.\n",
    "\n",
    "**Star dataset**\n",
    "- The model's accuracy, precision and recall scores are consistent across both the training and test sets, with the model scoring only slighter better on the training set across all metrics.\n",
    "- The scores indicate that the model has neither over- or under- fitted on the training data and can generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbaffb",
   "metadata": {},
   "source": [
    "### 70-30 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a07351",
   "metadata": {},
   "source": [
    "#### Splitting Datasets into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d91ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting productivity dataset\n",
    "gwp_mm.split_dataset(train_size=0.7, test_size=0.3)\n",
    "\n",
    "# Splitting star dataset\n",
    "star_mm.split_dataset(train_size=0.014, test_size=0.006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e5ad9",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d76b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset\n",
    "gwp_mm.train_model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.train_model_clf(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2a1f8",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "gwp_mm.visualise_results_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.visualise_results_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2582a",
   "metadata": {},
   "source": [
    "#### Analysis (INCOMPLETE)\n",
    "**Productivity dataset**\n",
    "- The model acheives a satisfactorally low score across all error metrics (MSE and MAE) in both the training and test sets.\n",
    "- However, the R2 score suggests that the model is unlikely to generalise well to novel data and may therefore require more fine-tuning. \n",
    "- The R2 score is particularly low on the test set, which could be an indication of underfitting.\n",
    "\n",
    "**Star dataset**\n",
    "- The model's accuracy, precision and recall scores are consistent across both the training and test sets, with the model scoring only slighter better on the training set across all metrics.\n",
    "- The scores indicate that the model has neither over- or under- fitted on the training data and can generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33841af0",
   "metadata": {},
   "source": [
    "### 60-40 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a85f51",
   "metadata": {},
   "source": [
    "#### Splitting Datasets into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ab19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting productivity dataset\n",
    "gwp_mm.split_dataset(train_size=0.6, test_size=0.4)\n",
    "\n",
    "# Splitting star dataset\n",
    "star_mm.split_dataset(train_size=0.012, test_size=0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a202f1d",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d579be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset\n",
    "gwp_mm.train_model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.train_model_clf(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecd3eb",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "gwp_mm.visualise_results_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d148aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.visualise_results_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f9bd5",
   "metadata": {},
   "source": [
    "#### Analysis (INCOMPLETE)\n",
    "**Productivity dataset**\n",
    "- The model acheives a satisfactorally low score across all error metrics (MSE and MAE) in both the training and test sets.\n",
    "- However, the R2 score suggests that the model is unlikely to generalise well to novel data and may therefore require more fine-tuning. \n",
    "- The R2 score is particularly low on the test set, which could be an indication of underfitting.\n",
    "\n",
    "**Star dataset**\n",
    "- The model's accuracy, precision and recall scores are consistent across both the training and test sets, with the model scoring only slighter better on the training set across all metrics.\n",
    "- The scores indicate that the model has neither over- or under- fitted on the training data and can generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c99a7",
   "metadata": {},
   "source": [
    "### 50-50 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880a4c9",
   "metadata": {},
   "source": [
    "#### Splitting Datasets into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting productivity dataset\n",
    "gwp_mm.split_dataset(train_size=0.5, test_size=0.5)\n",
    "\n",
    "# Splitting star dataset\n",
    "star_mm.split_dataset(train_size=0.01, test_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4b062",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eeaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productivity dataset\n",
    "gwp_mm.train_model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.train_model_clf(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34062493",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "gwp_mm.visualise_results_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Star dataset\n",
    "star_mm.visualise_results_clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fef591",
   "metadata": {},
   "source": [
    "#### Analysis (INCOMPLETE)\n",
    "**Productivity dataset**\n",
    "- The model acheives a satisfactorally low score across all error metrics (MSE and MAE) in both the training and test sets.\n",
    "- However, the R2 score suggests that the model is unlikely to generalise well to novel data and may therefore require more fine-tuning. \n",
    "- The R2 score is particularly low on the test set, which could be an indication of underfitting.\n",
    "\n",
    "**Star dataset**\n",
    "- The model's accuracy, precision and recall scores are consistent across both the training and test sets, with the model scoring only slighter better on the training set across all metrics.\n",
    "- The scores indicate that the model has neither over- or under- fitted on the training data and can generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b78ea",
   "metadata": {},
   "source": [
    "### Analysis of split ratios (INCOMPLETE)\n",
    "**Productivity dataset**\n",
    "- The overall accuracy of the model is relatively unaffected by the changing of split ratio. The MSE and MAE scores are fairly consistent across all split ratios with only a slight uptick as split ratio goes from 80-20 to 50-50.\n",
    "- The R2 scores are very low for all split ratios but they vary arbitrarily. A robust relationship/correlation cannot be determined; this may require further investigation.\n",
    "- 80-20 appears to be the optimal split ratio.\n",
    "\n",
    "**Star dataset**\n",
    "- Whilst model performance (across all metrics) is adequate for all split ratios, the gap in performance on the training set and test set begins to narrow slight as the split ratio approaches 50-50, with overall performance decreasing as well.; this suggests that as the model is fed less and less training data, is starts to underfit.\n",
    "- The true-positive rate for all classes remains fairly consistent as the split ratio approaches 50-50, with the 80-20 split having the slight edge.\n",
    "- Conversely, the average precision for classes 1 and 2 picks up slightly as the ratio approaches 50-50; class remains fairly consistent. This would suggest that split ratios closer to 50-50 have a better distribution of all the classes in the dataset.\n",
    "- The optimal split ratio appears to be 80-20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccc845",
   "metadata": {},
   "source": [
    "# Mardown Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e1a7e",
   "metadata": {},
   "source": [
    "Something..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c62029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
