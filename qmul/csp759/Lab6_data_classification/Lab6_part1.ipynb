{"cells":[{"cell_type":"markdown","metadata":{"id":"5Tv_COI0nhkS","tags":[]},"source":["# ECS759P Lab6 Part 1: Automated Classification of Data\n","\n","This lab focuses on data classification using decision trees and more particularly the ID3 algorithm.\n","\n","## Problem definition\n","\n","You are a data analyst in a company manufacturing and selling a wide range of tires. The research team came up with a new kind of rubber that improves the safety of the vehicles as well as the durability. However, its production is quite expensive and they are not sure whether it is profitable to create a new product or not. While discussing, you understand that this rubber is only useful for people who tend to drive aggressively. You decide then to use a part of your database containing your clients (which are car-making companies) and their products and label them to see if the new tires would fit. Your assumption is that some cars are meant to be driven more aggressively than others.\n","\n","## Data\n","\n","Run the following cell to load the data in the variable named `all_data`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLQZkC9_cAcx"},"outputs":[],"source":["import numpy as np\n","all_data = np.array([[\"A\", True, \t\"2\"\t,False, \t\"3\",\t\"2\", \"Diesel\", False],\n","[\"B\", True, \t\"4\"\t,False, \t\"3\",\t\"4\", \"Petrol\", True],\n","[\"A\", True, \t\"2\"\t,True, \t\"3\",\t\"2\", \"Petrol\", True],\n","[\"C\", False, \t\"2\"\t,True, \t\"3\",\t\"4\", \"Petrol\", False],\n","[\"B\", False, \t\"4\"\t,False, \t\"5\",\t\"4\", \"Diesel\", True],\n","[\"A\", True, \t\"2\"\t,False, \t\"3\",\t\"2\", \"Diesel\", False],\n","[\"B\", True, \t\"4\"\t,True, \t\"5\",\t\"4\", \"Petrol\", True],\n","[\"B\", False, \t\"2\"\t,False, \t\"5\",\t\"2\", \"Diesel\", False],\n","[\"A\", True, \t\"4\"\t,True, \t\"5\",\t\"4\", \"Petrol\", False],\n","[\"C\", False, \t\"4\"\t,True, \t\"3\",\t\"2\", \"Diesel\", True],\n","[\"A\", False, \t\"2\"\t,False, \t\"5\",\t\"2\", \"Petrol\", True],\n","[\"C\", False, \t\"2\"\t,False, \t\"3\",\t\"4\", \"Diesel\", True],\n","[\"B\", True, \t\"4\"\t,True, \t\"5\",\t\"4\", \"Petrol\", False],\n","[\"B\", True, \t\"2\"\t,True, \t\"5\",\t\"2\", \"Diesel\", True],\n","[\"C\", False, \t\"4\"\t,False, \t\"3\",\t\"4\", \"Petrol\", False],\n","[\"B\", True, \t\"2\"\t,False, \t\"5\",\t\"4\", \"Diesel\", False],\n","[\"A\", True, \t\"4\"\t,False, \t\"5\",\t\"2\", \"Diesel\", False],\n","[\"B\", False, \t\"2\"\t,False, \t\"3\",\t\"2\", \"Petrol\", False],\n","[\"B\", True, \t\"4\"\t,False, \t\"3\",\t\"2\", \"Petrol\", True],\n","[\"C\", True, \t\"2\"\t,False, \t\"5\",\t\"4\", \"Diesel\", False],\n","[\"A\", False, \t\"2\"\t,False, \t\"5\",\t\"4\", \"Diesel\", True],\n","[\"B\", True, \t\"2\"\t,True, \t\"3\",\t\"4\", \"Petrol\", True],\n","[\"C\", True, \t\"2\"\t,False, \t\"5\",\t\"2\", \"Diesel\", False],\n","[\"C\", True, \t\"2\"\t,True, \t\"5\",\t\"2\", \"Petrol\", False],\n","[\"A\", False, \t\"2\"\t,False, \t\"3\",\t\"4\", \"Diesel\", True],\n","[\"A\", True, \t\"2\"\t,True, \t\"5\",\t\"2\", \"Petrol\", False],\n","[\"B\", True, \t\"2\"\t,True, \t\"5\",\t\"4\", \"Petrol\", True],\n","[\"C\", True, \t\"4\"\t,False, \t\"3\",\t\"4\", \"Petrol\", False],\n","[\"A\", False, \t\"2\"\t,False, \t\"3\",\t\"2\", \"Diesel\", True],\n","[\"A\", True, \t\"4\"\t,True, \t\"5\",\t\"2\", \"Petrol\", False],\n","[\"C\", True, \t\"4\"\t,False, \t\"5\",\t\"2\", \"Diesel\", True],\n","[\"B\", False, \t\"2\"\t,True, \t\"3\",\t\"4\", \"Petrol\", True],\n","[\"B\", True, \t\"4\"\t,True, \t\"5\",\t\"2\", \"Petrol\", False],\n","[\"C\", False, \t\"4\"\t,True, \t\"3\",\t\"4\", \"Petrol\", True],\n","[\"B\", False, \t\"2\"\t,True, \t\"5\",\t\"2\", \"Diesel\", False],\n","[\"B\", False, \t\"4\"\t,False, \t\"5\",\t\"4\", \"Petrol\", True],\n","[\"C\", False, \t\"2\"\t,True, \t\"3\",\t\"4\", \"Diesel\", True],\n","[\"A\", False, \t\"4\"\t,True, \t\"5\",\t\"2\", \"Diesel\", True],\n","[\"C\", True, \t\"2\"\t,False, \t\"3\",\t\"2\", \"Petrol\", False],\n","[\"A\", False, \t\"2\"\t,False, \t\"3\",\t\"2\", \"Petrol\", True]])"]},{"cell_type":"markdown","metadata":{"id":"b42VHri4FKPM"},"source":["If you want a nice visualisation you can use pandas:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUtWlnzoFJKv"},"outputs":[],"source":["import pandas as pd\n","\n","pandas_dataframe = pd.DataFrame(data=all_data, index=[\"product_{}\".format(i) for i in range(1,41)], columns=[\"Brand\", \"Sport\", \"Driving wheel\", \"Large trunk\", \"Doors\", \"Seats\", \"Motor\", \"Label\"])\n","pandas_dataframe"]},{"cell_type":"markdown","metadata":{"id":"xJgwl_ps2TxA"},"source":["Let's try to implement the ID3 algorithm that would allow to carry out classification. You have a lot of different ways to do so, but here are the most important steps you might want to remember in general:\n","*   Compute the current total entropy\n","*   Get the number of categories for a given attribute\n","*   Compute the entropy for each category of a given attribute\n","*   Count the number of occurrences of each label for a given value of an attribute\n","*   Remove the instances (rows) that are perfectly classified\n","*   Remove the attributes already used\n","*   Keep track of the *questions* asked to discriminate the data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_3hohBb3NWsE"},"source":["1. For the next steps of this lab, we are going to use the first half of the data (product 1 to 20) as the training set.\n","\n","**Q. By filling the gaps below, implement a function that expects a dataframe and a label header as the input and returns the entropy value. Validation: You should obtain 0.99277 as the entropy value for the specified training dataframe.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImMxCTmC_IXU"},"outputs":[],"source":["import math\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def get_entropy(df, label): # calculate total entropy\n","  entropy = 0\n","  # TO DO\n","\n","  return entropy\n","\n","print(get_entropy(pandas_dataframe.iloc[:20], 'Label'))\n"]},{"cell_type":"markdown","metadata":{"id":"A8gt8h8dcGeY"},"source":["2. Next, we are going to implement a function to calculate the information gain for a given feature.\n","\n","**Q. By filling the gap below, complete the following function that expects a dataframe, a feature and the label as inputs and returns the information gain. Hint: You might want to call the function that you implemented above. Validation: You should obtain 0.007299 as the information gain for the feature 'Motor', with the training dataframe as the input.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XMBkBMNdg3J"},"outputs":[],"source":["def information_gain(df, feature, label): # calculate information gain for the considered \"feature\"\n","  info = 0\n","  # TO DO\n","\n","  return gain\n","\n","print(information_gain(pandas_dataframe.iloc[:20], 'Motor', 'Label'))"]},{"cell_type":"markdown","metadata":{"id":"4r_8SAVXen-I"},"source":["3. Now, we are going to find the best feature (the one having the highest information gain) to expand the tree.\n","\n","**Q. Bu filling the gaps below, complete the following function that returns the best feature and the corresponding information gain value. Hint: You might want to call the function that you implemented above. Validation: You should find 'Brand' as the best feature and 0.02856 as its information gain, with the training dataframe as the input.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InyBfeWagb4G"},"outputs":[],"source":["def get_best_feature(df, label):  # select best feature, the one having the highest gain\n","  # TO DO\n","  return best_feature, gain\n","\n","print(get_best_feature(pandas_dataframe.iloc[:20], 'Label'))"]},{"cell_type":"markdown","metadata":{"id":"esxEc10fhQCz"},"source":["The next step is to construct the decision tree using the ID3 algorithm. In order to save time, we provide the following functions. Compare this implementation with your understanding of the ID3 algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94fFRHvFiA-M"},"outputs":[],"source":["def get_subdf(df, node, value):   # get the sub dataframe at a particular node, based on a feature value\n","  return df[df[node] == value].reset_index(drop=True)\n","\n","def buildID3(df, tree=None, label=None):\n","  if label == None:\n","    label = df.keys()[-1]   # assign the last column as the label if not explicitly specified\n","  node, _ = get_best_feature(df, label)\n","  if tree is None:    # initialize the tree using dict\n","    tree={}\n","    tree[node] = {}\n","  unique_vals = df[node].unique()\n","  for val in unique_vals:\n","    subdf = get_subdf(df, node, val)\n","    cls, counts = np.unique(subdf[label],return_counts=True)  # get the label and the counts of unique values\n","    dup_subdf = subdf.drop(label, axis=1)   # to check if duplicate rows occur in the sub database, once 'Label' column in not considered\n","    dup_subdf.drop_duplicates(keep=False, inplace=True) # if we end up with a sub database having duplicate rows without considering the label, we must terminate. Otherwise the recursive loop runs forever\n","    if len(counts)==1 or dup_subdf.empty: # these 2 conditions result in an end leaf\n","      tree[node][val] = cls[0]\n","    else:\n","      tree[node][val] = buildID3(subdf) # recursively call the function to expand the tree\n","  return tree\n"]},{"cell_type":"markdown","metadata":{"id":"6KrPK7bPi2xh"},"source":["We can now train the decision tree using our training data and pretty-print the tree in JSON format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9Jr0v7ejZ7G"},"outputs":[],"source":["import json\n","\n","train_df = pandas_dataframe.iloc[:20]   # only select first 20 instances for training\n","t = buildID3(train_df)\n","print(json.dumps(t, sort_keys=True, indent=4))  # for pretty-printing"]},{"cell_type":"markdown","metadata":{"id":"2JXzis20jumI"},"source":["4. Draw the decision tree on a paper based on the output above."]},{"cell_type":"markdown","metadata":{"id":"CC12qIl_j9hl"},"source":["5. We want to evaluate the training and test accuracies for the decision tree that we implemented above. The function below can be used to recursively traverse the tree and reach a leaf node, which will be the prediction. Think about how this function works. The decision tree that you drew on paper might help.\n","\n","**Q. By filling the gaps below, call this function and obtain the accuracies for the training set (product 1 to 20) and the test set (product 21 to 40).**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0tm5Gl5llm4"},"outputs":[],"source":["def evaluate_model(df_test, tree):\n","  def walk(row, node):\n","    for key, item in node.items():\n","          val = row[key]\n","          new_item = item[val]\n","          if type(new_item) is dict:\n","            return walk(row, new_item)  # recursively traverse the tree\n","          else:\n","            return new_item\n","  predictions = []  # predictions made by traversing the tree\n","  for index, row in df_test.iterrows():\n","    predictions.append(walk(row, tree))\n","  df_test['Predictions'] = predictions\n","  df_test['Correctly predicted'] = np.where(df_test['Label'] == df_test['Predictions'], True, False)\n","  accuracy = df_test['Correctly predicted'].sum()/len(df_test)\n","  return df_test, accuracy\n","\n","# TO DO"]},{"cell_type":"markdown","metadata":{"id":"b2S3fK7Y_yQx"},"source":["6. Now, let's imagine that you also have continuous data as attributes -- say the price of each car, how would you implement it? Just think about it (you don't need to code), i.e how would you implement the threshold choice?\n","\n","## Using already existing libraries\n","As you can guess, although the principle is quite simple, implementing a classification tree coping with a wide range of data is not straightforward. Fortunately, we can use an already existing implementation. Note that **scikit learn** does not support categorical data.\n","\n","We are going to the sklearn implementation of the Decision Tree algorithm (https://scikit-learn.org/stable/modules/tree.html)."]},{"cell_type":"markdown","metadata":{"id":"pBwO4w7rzBxF"},"source":["**Q. By filling the gaps below, split *all_data* into *train_set*, *train_labels*, *test_set* and *test_labels*. Use can use the `train_test_split` function from Scikit Learn. Transform the non-numerical labels to numerical labels. Use can use the `LabelEncoder` class from Scikit Learn. Remember that the column corresponding to labels should not be part of the features. Then create an [DecisionTreeClassifier(criterion=\"entropy\")](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) similar to the ID3 algorithm. Train it on the *train_set* and compute the accuracy on the *test_set*.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTjHuS-oNZKX"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn import preprocessing\n","\n","features = [\"Brand\", \"Sport\", \"Driving wheel\", \"Large trunk\", \"Doors\", \"Seats\", \"Motor\", \"Label\"]\n","\n","# TO DO\n","\n","accuracy = accuracy_score(test_labels, preds)\n","print('Test accuracy is {}%'.format(accuracy*100))"]},{"cell_type":"markdown","metadata":{"id":"T6iLvIkgNZh7"},"source":["9. Print out and discuss the new decision tree.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnjhpeCTIIOB"},"outputs":[],"source":["from sklearn.tree import plot_tree\n","plot_tree(classifier)"]},{"cell_type":"code","source":["from sklearn.tree import export_text\n","print(export_text(classifier, feature_names=[\"Brand\", \"Sport\", \"Driving wheel\", \"Large trunk\", \"Doors\", \"Seats\", \"Motor\"]))"],"metadata":{"id":"4kVH88wvim9A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F65xuHkXcTu1"},"source":["10. By changing the `random_state` the function below, use different splits of the data for training and testing.\n","\n","**Q. How does that impact the final accuracy? Explain why this is happening with Decision Trees.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spFKe10komvL"},"outputs":[],"source":["train_set, test_set, train_labels, test_labels = train_test_split(all_data[:,:-1], all_data[:,-1], train_size=20, shuffle=True, random_state=8) # try different random states\n","classifier = DecisionTreeClassifier(criterion=\"entropy\")\n","classifier.fit(train_set, train_labels, check_input=True)\n","preds = classifier.predict(test_set)\n","accuracy = accuracy_score(test_labels, preds)\n","print('Test accuracy is {}%'.format(accuracy*100))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}